{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M36_-d4XQndm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbzYSQpPQ1MC"
      },
      "outputs": [],
      "source": [
        "train_terms = pd.read_csv(\"Train/train_terms.tsv\",sep=\"\\t\")\n",
        "print(train_terms.shape)\n",
        "train_terms.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2j7tlPgRDbw"
      },
      "outputs": [],
      "source": [
        "train_protein_ids = np.load('train_ids.npy')\n",
        "print(train_protein_ids.shape)\n",
        "print(train_protein_ids[:5])\n",
        "\n",
        "train_embeddings = np.load('train_embeds.npy')\n",
        "\n",
        "# Now lets convert embeddings numpy array(train_embeddings) into pandas dataframe.\n",
        "column_num = train_embeddings.shape[1]\n",
        "train_df = pd.DataFrame(train_embeddings, columns = [\"Column_\" + str(i) for i in range(1, column_num+1)])\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQRNphoyTbYL"
      },
      "outputs": [],
      "source": [
        "print(train_terms['term'].value_counts().shape)\n",
        "\n",
        "# Select first 1500 values for plotting\n",
        "plot_df = train_terms['term'].value_counts().iloc[:100]\n",
        "\n",
        "figure, axis = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "bp = sns.barplot(ax=axis, x=np.array(plot_df.index), y=plot_df.values)\n",
        "bp.set_xticklabels(bp.get_xticklabels(), rotation=90, size = 6)\n",
        "axis.set_title('Top 100 frequent GO term IDs')\n",
        "bp.set_xlabel(\"GO term IDs\", fontsize = 12)\n",
        "bp.set_ylabel(\"Count\", fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoOx2lL-TfiM"
      },
      "outputs": [],
      "source": [
        "# Set the limit for label\n",
        "num_of_labels = 500\n",
        "\n",
        "# Take value counts in descending order and fetch first 1500 `GO term ID` as labels\n",
        "labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()\n",
        "\n",
        "# Fetch the train_terms data for the relevant labels only\n",
        "train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]\n",
        "print(train_terms_updated.shape)\n",
        "\n",
        "pie_df = train_terms_updated['aspect'].value_counts()\n",
        "palette_color = sns.color_palette('bright')\n",
        "plt.pie(pie_df.values, labels=np.array(pie_df.index), colors=palette_color, autopct='%.0f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz-6Coj5Ts4r"
      },
      "outputs": [],
      "source": [
        "# Create an empty dataframe of required size for storing the labels,\n",
        "# i.e, train_size x num_of_labels (142246 x 1500)\n",
        "train_size = train_protein_ids.shape[0] # len(X)\n",
        "train_labels = np.zeros((train_size ,num_of_labels))\n",
        "\n",
        "# Convert from numpy to pandas series for better handling\n",
        "series_train_protein_ids = pd.Series(train_protein_ids)\n",
        "\n",
        "# Loop through each label\n",
        "for i in tqdm(range(num_of_labels)):\n",
        "    # For each label, fetch the corresponding train_terms data\n",
        "    n_train_terms = train_terms_updated[train_terms_updated['term'] ==  labels[i]]\n",
        "\n",
        "    # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n",
        "    label_related_proteins = n_train_terms['EntryID'].unique()\n",
        "\n",
        "    # In the series_train_protein_ids pandas series, if a protein is related\n",
        "    # to the current label, then mark it as 1, else 0.\n",
        "    # Replace the ith column of train_Y with with that pandas series.\n",
        "    train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n",
        "\n",
        "# Convert train_Y numpy into pandas dataframe\n",
        "labels_df = pd.DataFrame(data = train_labels, columns = labels)\n",
        "print(labels_df.shape)\n",
        "labels_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sioc6oGMWSyn"
      },
      "outputs": [],
      "source": [
        "labels_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PA-A2NAZApT"
      },
      "outputs": [],
      "source": [
        "labels_df.to_csv('labels_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoWjX7EwZxJZ"
      },
      "outputs": [],
      "source": [
        "labels_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr = train_df.corr()\n",
        "means_of_correlations = abs(corr).mean()\n",
        "plt.hist(means_of_correlations,bins=100)\n",
        "plt.show()\n",
        "highly_correlated_cols= np.where(means_of_correlations>0.12)\n",
        "print('number of bad cols:' ,len(highly_correlated_cols[0]))\n",
        "print('col numbers:',(highly_correlated_cols[0]+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reduced_train_df = train_df.drop(train_df.columns[highly_correlated_cols], axis =1)\n",
        "reduced_train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG6g_TthoBAv"
      },
      "outputs": [],
      "source": [
        "#PCA with threshold:\n",
        "def apply_pca_with_threshold(data, threshold):\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA()\n",
        "    pca_result = pca.fit_transform(scaled_data)\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of components')\n",
        "    plt.ylabel('Cumulative explained variance')\n",
        "    plt.show()\n",
        "    # Get the explained variance ratios\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "    # Determine the number of components to retain based on the threshold\n",
        "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "    num_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
        "\n",
        "    # Fit PCA with the selected number of components\n",
        "    pca_selected = PCA(n_components=num_components)\n",
        "    pca_result_selected = pca_selected.fit_transform(scaled_data)\n",
        "\n",
        "    # Transform the selected components back to the original space\n",
        "    selected_features = pca_selected.inverse_transform(pca_result_selected)\n",
        "\n",
        "    # Create a DataFrame with the selected features\n",
        "    selected_df = pd.DataFrame(data=selected_features, columns=data.columns)\n",
        "\n",
        "    # Identify the dropped columns\n",
        "    dropped_columns = data.columns.difference(selected_df.columns)\n",
        "\n",
        "    pca_threshold = pd.DataFrame(data=selected_features, columns=train_df.columns)\n",
        "\n",
        "    return num_components, dropped_columns, pca_threshold\n",
        "\n",
        "threshold = 0.99\n",
        "\n",
        "# Apply PCA with the specified threshold\n",
        "num_components_retained, dropped_columns, pca_threshold_df = apply_pca_with_threshold(train_df, threshold)\n",
        "print(\"Number of components retained:\", num_components_retained)\n",
        "print(pca_threshold_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_SHAPE = [pca_threshold_df.shape[1]]\n",
        "BATCH_SIZE = 5120\n",
        "epochs = 200\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.BatchNormalization(input_shape=INPUT_SHAPE),\n",
        "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=num_of_labels,activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy', tf.keras.metrics.AUC()],\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=10,          # Number of epochs with no improvement before stopping\n",
        "    restore_best_weights=True  # Restore weights from the epoch with the best validation loss\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    pca_threshold_df, labels_df,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=epochs\n",
        "    callbacks=[early_stopping]  \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = model.predict(np.array(pca_threshold_df)[0][np.newaxis,...])\n",
        "label  = np.array(labels_df)[0]\n",
        "print(pred)\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = model.evaluate(pca_threshold_df, labels_df)\n",
        "accuracy = evaluation[1]  # binary_accuracy\n",
        "print(\"Overall Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
